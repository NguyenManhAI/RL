{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# residual for MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResMLP Architecture\n",
    "![ResMLP Architecture](../ResMLP-architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlockResMLP(nn.Module):\n",
    "    def __init__(self, in_features, outs_features) -> None:\n",
    "        super().__init__()\n",
    "        features = [in_features] + outs_features\n",
    "        self.block = nn.Sequential(\n",
    "            *[nn.Sequential(nn.Linear(ins, outs), nn.ReLU() if i < len(features) - 2 else nn.Identity())\n",
    "              for i, (ins, outs) in enumerate(zip(features[:-1], features[1:]))]\n",
    "        )\n",
    "        self.skip_conection = nn.Linear(in_features, outs_features[-1])\n",
    "    def forward(self,X):\n",
    "        out = self.block(X)\n",
    "        out += self.skip_conection(X)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "def test_BlockResMLP():\n",
    "    config = {\n",
    "        'in_features': 9,\n",
    "        'outs_features': [32,16,32],\n",
    "    }\n",
    "    X = torch.Tensor(4,9)\n",
    "    model = BlockResMLP(**config)\n",
    "    out = model(X)\n",
    "    print(out.shape)\n",
    "    print(model)\n",
    "# test_BlockResMLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerResMLP(nn.Module):\n",
    "    def __init__(self, num_blocks, in_features, outs_features) -> None:\n",
    "        super().__init__()\n",
    "        ins_feature = [in_features] + [outs_features[-1]]*(num_blocks-1)\n",
    "        self.layer = nn.Sequential(\n",
    "            *[BlockResMLP(in_features= ins, outs_features= outs_features) for ins in ins_feature]\n",
    "        )\n",
    "    def forward(self, X):\n",
    "        out = self.layer(X)\n",
    "        return out\n",
    "def test_LayerResMLP():\n",
    "    config = {\n",
    "        'num_blocks': 2,\n",
    "        'in_features': 9,\n",
    "        'outs_features': [32,16,32],\n",
    "    }\n",
    "    X = torch.Tensor(4,9)\n",
    "    model = LayerResMLP(**config)\n",
    "    out = model(X)\n",
    "    print(out.shape)\n",
    "    print(model)\n",
    "# test_LayerResMLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'layer1':{\n",
    "        'num_blocks': 2,\n",
    "        'in_features': 9,\n",
    "        'outs_features': [32,16,32],\n",
    "    },\n",
    "    'layer2':{\n",
    "        'num_blocks': 2,\n",
    "        'in_features': 32,\n",
    "        'outs_features': [64,32,64],\n",
    "    },\n",
    "    'layer3':{\n",
    "        'num_blocks': 2,\n",
    "        'in_features': 64,\n",
    "        'outs_features': [128,64,128],\n",
    "    },\n",
    "    'layer4':{\n",
    "        'num_blocks': 2,\n",
    "        'in_features': 128,\n",
    "        'outs_features': [256,128,256],\n",
    "    },\n",
    "    'layer_out':{\n",
    "        'in_features': 256,\n",
    "        'out_features': 9 \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 9])\n",
      "ResMLP(\n",
      "  (model): Sequential(\n",
      "    (0): LayerResMLP(\n",
      "      (layer): Sequential(\n",
      "        (0): BlockResMLP(\n",
      "          (block): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Linear(in_features=9, out_features=32, bias=True)\n",
      "              (1): ReLU()\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Linear(in_features=32, out_features=16, bias=True)\n",
      "              (1): ReLU()\n",
      "            )\n",
      "            (2): Sequential(\n",
      "              (0): Linear(in_features=16, out_features=32, bias=True)\n",
      "              (1): Identity()\n",
      "            )\n",
      "          )\n",
      "          (skip_conection): Linear(in_features=9, out_features=32, bias=True)\n",
      "        )\n",
      "        (1): BlockResMLP(\n",
      "          (block): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "              (1): ReLU()\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Linear(in_features=32, out_features=16, bias=True)\n",
      "              (1): ReLU()\n",
      "            )\n",
      "            (2): Sequential(\n",
      "              (0): Linear(in_features=16, out_features=32, bias=True)\n",
      "              (1): Identity()\n",
      "            )\n",
      "          )\n",
      "          (skip_conection): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): LayerResMLP(\n",
      "      (layer): Sequential(\n",
      "        (0): BlockResMLP(\n",
      "          (block): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Linear(in_features=32, out_features=64, bias=True)\n",
      "              (1): ReLU()\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Linear(in_features=64, out_features=32, bias=True)\n",
      "              (1): ReLU()\n",
      "            )\n",
      "            (2): Sequential(\n",
      "              (0): Linear(in_features=32, out_features=64, bias=True)\n",
      "              (1): Identity()\n",
      "            )\n",
      "          )\n",
      "          (skip_conection): Linear(in_features=32, out_features=64, bias=True)\n",
      "        )\n",
      "        (1): BlockResMLP(\n",
      "          (block): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "              (1): ReLU()\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Linear(in_features=64, out_features=32, bias=True)\n",
      "              (1): ReLU()\n",
      "            )\n",
      "            (2): Sequential(\n",
      "              (0): Linear(in_features=32, out_features=64, bias=True)\n",
      "              (1): Identity()\n",
      "            )\n",
      "          )\n",
      "          (skip_conection): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): LayerResMLP(\n",
      "      (layer): Sequential(\n",
      "        (0): BlockResMLP(\n",
      "          (block): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "              (1): ReLU()\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Linear(in_features=128, out_features=64, bias=True)\n",
      "              (1): ReLU()\n",
      "            )\n",
      "            (2): Sequential(\n",
      "              (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "              (1): Identity()\n",
      "            )\n",
      "          )\n",
      "          (skip_conection): Linear(in_features=64, out_features=128, bias=True)\n",
      "        )\n",
      "        (1): BlockResMLP(\n",
      "          (block): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (1): ReLU()\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Linear(in_features=128, out_features=64, bias=True)\n",
      "              (1): ReLU()\n",
      "            )\n",
      "            (2): Sequential(\n",
      "              (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "              (1): Identity()\n",
      "            )\n",
      "          )\n",
      "          (skip_conection): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): LayerResMLP(\n",
      "      (layer): Sequential(\n",
      "        (0): BlockResMLP(\n",
      "          (block): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "              (1): ReLU()\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "              (1): ReLU()\n",
      "            )\n",
      "            (2): Sequential(\n",
      "              (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "              (1): Identity()\n",
      "            )\n",
      "          )\n",
      "          (skip_conection): Linear(in_features=128, out_features=256, bias=True)\n",
      "        )\n",
      "        (1): BlockResMLP(\n",
      "          (block): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (1): ReLU()\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "              (1): ReLU()\n",
      "            )\n",
      "            (2): Sequential(\n",
      "              (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "              (1): Identity()\n",
      "            )\n",
      "          )\n",
      "          (skip_conection): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): Linear(in_features=256, out_features=9, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class ResMLP(nn.Module):\n",
    "    def __init__(self, config) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        layer = []\n",
    "        for i in range(1, len(config)):\n",
    "            config_params_layer = config[f'layer{i}']\n",
    "            layer.append(LayerResMLP(**config_params_layer))\n",
    "        \n",
    "        config_params_layer_out = config['layer_out']\n",
    "        layer.append(nn.Linear(**config_params_layer_out))\n",
    "        \n",
    "        self.model = nn.Sequential(*layer)\n",
    "\n",
    "    def forward(self,X):\n",
    "        out = self.model(X)\n",
    "        return out\n",
    "\n",
    "def test_ResMLP():\n",
    "    X = torch.Tensor(4,9)\n",
    "    model = ResMLP(config)\n",
    "    out = model(X)\n",
    "    print(out.shape)\n",
    "    print(model)\n",
    "test_ResMLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "manhtms1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
