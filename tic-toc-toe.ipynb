{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Để cho đơn giản, máy chọn X, đối phương chọn O\\nX đi trước\\n\\ntác tử (Agent): X\\nhành động (action A): thêm X vào bảng\\ntín hiệu phần thưởng (Reward signal R): R = {-1: thua, 1: thắng, 0: hòa, hoặc chưa kết thúc}\\nlợi tức (return G): kì vọng tổng các tín hiệu phần thưởng có discount\\ntrạng thái (state S): \\n    tùy thuộc vào đối phương, mỗi nước đi của X sản sinh ra nhiều trạng thái khác nhau\\n    thời điểm t -> t+1 là thời điểm thêm X vào bảng và tạo ra môi trường có X vừa thêm và O do đối thủ thêm vào, như vậy có thể coi t = số X có trong bảng\\n    Giả sử mỗi khi thêm X, xác suất xảy ra của mỗi trạng thái là như nhau\\n    cần mã hóa bảng trước khi cho vào hàm giá trị để giảm không gian lưu trữ\\n    Định nghĩa trạng thái kết thúc.\\n\\nhàm giá trị (value function) v(s)\\nchiến lược: đi vào trạng thái có hàm giá trị lớn nhất\\n\\nCác đối tượng:\\n- board\\n- Agent\\n'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Để cho đơn giản, máy chọn X, đối phương chọn O\n",
    "X đi trước\n",
    "\n",
    "tác tử (Agent): X\n",
    "hành động (action A): thêm X vào bảng\n",
    "tín hiệu phần thưởng (Reward signal R): R = {-1: thua, 1: thắng, 0: hòa, hoặc chưa kết thúc}\n",
    "lợi tức (return G): kì vọng tổng các tín hiệu phần thưởng có discount\n",
    "trạng thái (state S): \n",
    "    tùy thuộc vào đối phương, mỗi nước đi của X sản sinh ra nhiều trạng thái khác nhau\n",
    "    thời điểm t -> t+1 là thời điểm thêm X vào bảng và tạo ra môi trường có X vừa thêm và O do đối thủ thêm vào, như vậy có thể coi t = số X có trong bảng\n",
    "    Giả sử mỗi khi thêm X, xác suất xảy ra của mỗi trạng thái là như nhau\n",
    "    cần mã hóa bảng trước khi cho vào hàm giá trị để giảm không gian lưu trữ\n",
    "    Định nghĩa trạng thái kết thúc.\n",
    "\n",
    "hàm giá trị (value function) v(s)\n",
    "chiến lược: đi vào trạng thái có hàm giá trị lớn nhất\n",
    "\n",
    "Các đối tượng:\n",
    "- board\n",
    "- Agent\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import hashlib\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kernel:\n",
    "    def __init__(self, is_setup_vp = True) -> None:\n",
    "        self.H = 3\n",
    "        self.W = 3\n",
    "\n",
    "        self.value_function = {}\n",
    "\n",
    "        if is_setup_vp:\n",
    "            self.setup_vp(np.zeros((3,3)))\n",
    "    \n",
    "    def setup_vp(self, state) -> None:\n",
    "        '''\n",
    "        Thêm state hiện tại và các state phía sau\n",
    "        B1: Thêm state hiện tại\n",
    "            Nếu state hiện tại đã có, nên nó và những state sau nó cũng đã có -> dừng\n",
    "        B2: Nếu state hiện tại là terminal -> dừng, nếu không, tiếp tục thêm các state phía sau\n",
    "        '''\n",
    "        \n",
    "        # Thêm state hiện tại:\n",
    "        if self.value_function.get(tuple(map(tuple, state))) is None:\n",
    "            self.value_function[tuple(map(tuple, state))] = 0\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "        # kiểm tra terminal\n",
    "        if self.is_terminal(state):\n",
    "            return None\n",
    "        \n",
    "        nextActions = self.get_nextActions(state)\n",
    "        for na in nextActions:\n",
    "            nextStates = self.get_nextStates(na)\n",
    "            if not nextStates:\n",
    "                self.setup_vp(na)\n",
    "            for ns in nextStates:\n",
    "                self.setup_vp(ns)\n",
    "\n",
    "    def reward_signal_kernel(self,kernel):#3x3\n",
    "        # check rows\n",
    "        for row in kernel:\n",
    "            if row[0] == row[1] == row[2] and row[0] != 0:\n",
    "                return row[0]\n",
    "        # check columns:\n",
    "        for col in range(3):\n",
    "            if kernel[0][col] == kernel[1][col] == kernel[2][col] and kernel[0][col] != 0:\n",
    "                return kernel[0][col]\n",
    "        # check main diagonal\n",
    "        if kernel[0][0] == kernel[1][1] == kernel[2][2] and kernel[0][0] != 0:\n",
    "            return kernel[0][0]\n",
    "        # check secondary diagonal\n",
    "        if kernel[0][2] == kernel[1][1] == kernel[2][0] and kernel[0][2] != 0:\n",
    "            return kernel[0][2]\n",
    "        \n",
    "        return 0\n",
    "    \n",
    "    def is_terminal(self, board):\n",
    "        r = self.reward_signal_kernel(board)\n",
    "        if r != 0:\n",
    "            return True\n",
    "        else:\n",
    "            if 0 in board:\n",
    "                return False\n",
    "            else:\n",
    "                return True\n",
    "            \n",
    "    def get_nextActions(self, state):\n",
    "        # kiểm tra state đã kết thúc hoặc không thêm được X\n",
    "        if self.is_terminal(state):\n",
    "            return []\n",
    "        idxs_empty = []\n",
    "        \n",
    "        # Tìm tất cả các ô trống (0)\n",
    "        for i in range(self.H):\n",
    "            for j in range(self.W):\n",
    "                if state[i][j] == 0:\n",
    "                    idxs_empty.append((i,j))\n",
    "\n",
    "        if not idxs_empty:\n",
    "            return []\n",
    "        \n",
    "        nextActions = [state]\n",
    "        for i,j in idxs_empty:\n",
    "            new_action = np.array([row[:] for row in state])\n",
    "            new_action[i][j] = 1\n",
    "            nextActions.append(new_action)\n",
    "\n",
    "        return nextActions\n",
    "    \n",
    "    def get_nextStates(self, state):\n",
    "        # kiểm tra state đã kết thúc hoặc không thêm được 0\n",
    "        if self.is_terminal(state):\n",
    "            return []\n",
    "        idxs_empty = []\n",
    "        \n",
    "        # Tìm tất cả các ô trống (0)\n",
    "        for i in range(self.H):\n",
    "            for j in range(self.W):\n",
    "                if state[i][j] == 0:\n",
    "                    idxs_empty.append((i,j))\n",
    "\n",
    "        if not idxs_empty:\n",
    "            return []\n",
    "        \n",
    "        nextStates = [state]\n",
    "        for i,j in idxs_empty:\n",
    "            new_state = np.array([row[:] for row in state])\n",
    "            new_state[i][j] = -1\n",
    "            nextStates.append(new_state)\n",
    "\n",
    "        return nextStates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player:\n",
    "    def __init__(self) -> None:\n",
    "        self.kernel = Kernel(is_setup_vp=False)\n",
    "        pass\n",
    "    def get_final_score(self, board):\n",
    "        H, W = board.shape\n",
    "        for i in range(H - 2):\n",
    "            for j in range(W - 2):\n",
    "                r = self.kernel.reward_signal_kernel(board[i:i+3, j:j+3])\n",
    "                if r != 0:\n",
    "                    return r\n",
    "        return 0\n",
    "\n",
    "    def check_terminal_board(self, board):\n",
    "        H, W = board.shape\n",
    "        for i in range(H - 2):\n",
    "            for j in range(W - 2):\n",
    "                r = self.kernel.reward_signal_kernel(board[i:i+3, j:j+3])\n",
    "                if r != 0:\n",
    "                    return True\n",
    "        if 0 in board:#chưa kết thúc\n",
    "            return False\n",
    "        else:# hòa\n",
    "            return True\n",
    "    def predict(self, board):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(Player):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.theta = 10e-3\n",
    "        self.gamma = 0.9\n",
    "        self.kernel = Kernel()\n",
    "    \n",
    "    def train(self):\n",
    "        epoch = 0\n",
    "\n",
    "        while(True):\n",
    "            delta = 0\n",
    "            for t_s in self.kernel.value_function:\n",
    "                s = np.array(t_s)\n",
    "                if self.kernel.is_terminal(s):\n",
    "                    self.kernel.value_function[tuple(map(tuple, s))] = self.kernel.reward_signal_kernel(s)\n",
    "                else:\n",
    "                    q = {}\n",
    "                    actions = self.kernel.get_nextActions(s)\n",
    "                    for a in actions:\n",
    "                        if self.kernel.is_terminal(a):\n",
    "                            q[tuple(map(tuple, a))] = self.kernel.reward_signal_kernel(a)\n",
    "                            continue\n",
    "\n",
    "                        next_states = self.kernel.get_nextStates(a)\n",
    "                        q_a = []\n",
    "                        for ns in next_states:\n",
    "                            v_ns = self.kernel.value_function.get(tuple(map(tuple, ns)))\n",
    "                            q_a.append(self.kernel.reward_signal_kernel(ns) + self.gamma*v_ns)\n",
    "                        q_a = np.mean(q_a)\n",
    "                        q[tuple(map(tuple, a))] = q_a\n",
    "\n",
    "                    pi_s = max(q, key=q.get)\n",
    "\n",
    "                    v = self.kernel.value_function.get(tuple(map(tuple, s)))\n",
    "                    new_v = q[pi_s]\n",
    "\n",
    "                    self.kernel.value_function[tuple(map(tuple, s))] = new_v\n",
    "\n",
    "                    delta = max(delta, abs(v - new_v))\n",
    "            epoch += 1\n",
    "            print(f\"epoch{epoch}: delta={delta}\")\n",
    "            if epoch % 1 == 0:\n",
    "                a = np.array([[-1,-1,0],[0,0,0],[0,0,0]])\n",
    "                print(self.kernel.value_function.get(tuple(map(tuple, a))))\n",
    "            if delta < self.theta or epoch == 1000:\n",
    "                break\n",
    "        \n",
    "        return self.kernel.value_function\n",
    "\n",
    "    def predict(self, board):\n",
    "        state = board\n",
    "        H, W = board.shape\n",
    "        if self.check_terminal_board(board):\n",
    "            return None\n",
    "        # dịch kernel\n",
    "        o_pi_kernel = None\n",
    "        o_v_kernel = 0\n",
    "        idx = None\n",
    "        for i in range(H - 2):\n",
    "            for j in range(W - 2):\n",
    "                kernel = state[i:i+3, j:j+3]\n",
    "                \n",
    "                q = {}\n",
    "                actions = self.kernel.get_nextActions(kernel)\n",
    "                for a in actions:\n",
    "                    if self.kernel.is_terminal(a):\n",
    "                        q[tuple(map(tuple, a))] = self.kernel.reward_signal_kernel(a)\n",
    "                        continue\n",
    "\n",
    "                    next_states = self.kernel.get_nextStates(a)\n",
    "                    q_a = []\n",
    "                    for ns in next_states:\n",
    "                        v_ns = self.kernel.value_function.get(tuple(map(tuple, ns)))\n",
    "                        q_a.append(self.kernel.reward_signal_kernel(ns) + self.gamma*v_ns)\n",
    "                    q_a = np.mean(q_a)\n",
    "                    q[tuple(map(tuple, a))] = q_a\n",
    "\n",
    "                pi_kernel = np.array(max(q, key=q.get))\n",
    "                q_a_kernel = q.get(tuple(map(tuple, pi_kernel)))\n",
    "                if q_a_kernel > o_v_kernel:\n",
    "                    o_v_kernel = q_a_kernel\n",
    "                    o_pi_kernel = pi_kernel\n",
    "                    idx = (i,j)\n",
    "\n",
    "        new_state = np.array([row[:] for row in state])\n",
    "        i,j = idx\n",
    "        new_state[i:i+3, j:j+3] = o_pi_kernel\n",
    "\n",
    "        return new_state\n",
    "    \n",
    "    def predict_opponent(self, board):\n",
    "        opt_board = board * -1\n",
    "        return self.predict(opt_board) * -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class RandomPlayer(Player):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        pass\n",
    "    def predict(self,board):\n",
    "        H, W = board.shape\n",
    "        if self.check_terminal_board(board):\n",
    "            return None\n",
    "        idxs_empty = []\n",
    "        \n",
    "        # Tìm tất cả các ô trống (0)\n",
    "        for i in range(H):\n",
    "            for j in range(W):\n",
    "                if board[i][j] == 0:\n",
    "                    idxs_empty.append((i,j))\n",
    "\n",
    "        if not idxs_empty:\n",
    "            return None\n",
    "        \n",
    "        random_idx = random.choice(idxs_empty)\n",
    "        new_board = np.array([row[:] for row in board])\n",
    "\n",
    "        i,j = random_idx\n",
    "        new_board[i][j] = -1\n",
    "\n",
    "        return new_board\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumanPlayer(Player):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        pass\n",
    "    def predict(self,board, i, j):\n",
    "        new_board = np.array([row[:] for row in board])\n",
    "\n",
    "        new_board[i][j] = -1\n",
    "\n",
    "        return new_board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game:\n",
    "    def __init__(self) -> None:\n",
    "        self.player = Player()\n",
    "        self.agent = Agent()\n",
    "        self.agent.train()\n",
    "        self.random_player = RandomPlayer()\n",
    "\n",
    "    def play(self, player1, player2, H, W):\n",
    "        board = np.zeros((H, W))\n",
    "        while(True):\n",
    "            board = player1.predict(board)\n",
    "            if self.player.check_terminal_board(board):\n",
    "                return self.player.get_final_score(board)\n",
    "            board = player2.predict(board)\n",
    "            if self.player.check_terminal_board(board):\n",
    "                return self.player.get_final_score(board)\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "manhtms1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
